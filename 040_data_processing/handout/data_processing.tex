\setModuleTitle{Trimming, Filtering and Pre-Processing NGS Data}
\setModuleAuthors{%
    Steve Pederson, Bioinformatics Hub, University of Adelaide \mailto{stephen.pederson@adelaide.edu.au}\\
}
\setModuleContributions{%
  Jimmy Breen, Robinson Research Institute \& Bioinformatics Hub, University of Adelaide \mailto{stephen.pederson@adelaide.edu.au}\\
  Dan Kortschak, Adelson Research Group, University of Adelaide \mailto{dan.kortschak@adelaide.edu.au} \\
  Terry Bertozzi, SA Museum \mailto{Terry.Bertozzi@samuseum.sa.gov.au}
}

\chapter{\moduleTitle}
\newpage

%% Spend more time on adapter trimming

Once we have inspected our data \& have an idea of how accurate our reads are, as well as any other technical issues that may be within the data, we may need to trim or filter the reads to make sure we are aligning or analysing sequences that accurately represent our source material.
As we've noticed, the quality of reads commonly drops off towards the end of the reads, and dealing with this behaviour is an important part of most processing pipelines.
Sometimes we will require reads of identical lengths for our downstream analysis, whilst other times we can use reads of varying lengths.
The data cleaning steps we choose for our own analysis will inevitably be influenced by our downstream requirements.

\section{The Basic Workflow}

Data cleaning \& pre-processing can involve many steps, and today we will use the basic work-flow as outlined in the following flow chart.
Every analysis is slightly different so some steps may or may not be required for your own data.
Some steps do have a little overlap, and some pipelines (e.g. \textit{Stacks}) may perform some of these steps for you.\\

Using today's datasets, we will take one sequencing through demultiplexing and adapter removal, and then use our C. elegans WGS to run genome mapping and alignment filtering.
We will perform most steps on files at this stage, rather than on a complete library, but the principle is essentially the same.\\

% Define block styles
\tikzstyle{block} = [rectangle, draw, fill=blue!20,  text width=4cm, text centered, rounded corners, minimum height=1.7cm]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, rectangle,fill=red!20, node distance=5cm, minimum height=1.2cm, minimum width=3cm,  text centered, rounded corners]

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance = 2.5cm, auto]

	    % Place nodes
	    \node [block] (lib) {Complete Library (i.e. 1 Flowcell Lane)};
	    \node [block, below of=lib] (flag) {Removal of Low Quality Reads};
	    \node [block, below of=flag] (adapt) {Remove Adapters};
	    \node [block, below of=adapt] (trim) {Read Trimming \&/or Filtering};
		\node [block, below of=trim] (demult) {Demultiplexing};
		\node [block, below of=demult] (align) {Alignment To Reference};
		\node [block, below of=align] (bam) {Filtering Post-Alignment};

		% Place Clouds
		\node [cloud, right of=flag] (filt) {\footnotesize{fastq\_illumina\_filter}};
		\node [cloud, right of=adapt] (cutadapt) {\footnotesize{cutadapt}};
		\node [cloud, right of=trim] (fastx) {
			\begin{tabular}{c}
			\footnotesize{fastq\_quality\_trimmer} \\
			\footnotesize{fastx\_trimmer}
			\end{tabular}};
		\node [cloud, right of=demult] (fastqm) { \footnotesize{fastq\_multx}};
		\node [cloud, right of=align] (tophat) {\footnotesize{tophat}};
		\node [cloud, right of=bam] (samtools) {\footnotesize{samtools view}};

	    % Draw edges
	    \path [line] (lib) -- (flag);
	    \path [line] (flag) -- (adapt);
	    \path [line] (adapt) -- (trim);
	    \path [line] (trim) -- (demult);
	    \path [line] (demult) -- (align);
	    \path [line] (align) -- (bam);
	    \path [line,dashed] (filt) -- (flag);
	    \path [line,dashed] (cutadapt) -- (adapt);
	    \path [line,dashed] (fastx) -- (trim);
	    \path [line,dashed](fastqm) -- (demult);
	    \path [line, dashed] (tophat) -- (align);
	    \path [line, dashed] (samtools) -- (bam);

	\end{tikzpicture}
\end{figure}

\section{Demultiplexing}

In the previous section "Understanding NGS Data \& FASTQ Format" we discussed the difference between an \textit{index} and a \textit{barcode}. If you use an indexed adapter to distinguish samples on an Illumina sequencing run, the demultiplexing is \textit{usually} done on the sequencing machine. However, sometimes it makes sense to use a barcode (or sometimes called "inline barcode"), to further multiplex samples onto one sequencing run.\\

\begin{note}
While barcode can be incredibly useful, it is important to note that Illumina cycle calibration and cluster calling is done in the first 4 cycles (first four base-pairs of read 1). It also is used to establish other metrics (e.g., signal thresholds) for base-calling.
Therefore it is essential that the first four base pairs are "diverse" (i.e. no particular nucleotide is over-represented in the first four base-pairs). Designing the right barcodes to add to the start of your reads extremely important! \\
Additionally, the Illumina NextSeq machine's have a slighly different sequence setup to the other sequencing machines (MiSeq, HiSeq and GAII's), using two-channel sequencing, which requires only two images to encode the data for four DNA bases, one red channel and one green channel. Guanine is an absence of colour and therefore at least one base other than G most be present in the first two cycles (\url{http://blog.kokocinski.net/index.php/barcode-balancing-for-illumina-sequencing?blog=2}). \\
For more information on Illumina cluster density and other technical aspects of cycles and imaging, be sure to read the following Illumina support material \url{https://support.illumina.com/content/dam/illumina-marketing/documents/products/other/miseq-overclustering-primer-770-2014-038.pdf}.
\end{note}

\begin{steps}
To demonstrate demultiplexing we will use the a sequencing run with two samples that have a 7bp barcode. Our barcode sequences should be "GCGTAGT" (for sample1) and "CCTCGTA" (for sample2). Lets first see what possible barcodes are available in the first 7bp of our dataset and see if it matches what we expect:
\begin{lstlisting}
cd ~/rawData/Multiplexed/Run1_R1.fastq.gz
zcat Run1_R1.fastq.gz | sed -n '2~4p' | cut -c 1-7 | sort | uniq -c | sort -nr | head -n10
\end{lstlisting}
\end{steps}

\begin{questions}
What top 5 barcodes are found in our data? Do the top two reflect our the barcodes we should have?\\
\begin{answer}
1577658 CCTCGTA and 1445792 GCGTAGT. These are the barcodes we need
\end{answer}
The command above is quite long and contains multiple unix commands that are separated by a pipe. What does each command do?
\begin{answer}
zcat Run1_R1.fastq.gz: Prints the compressed fastq file to screen \\
sed -n '2~4p': Prints the second line (sequence of each fastq file) \\
cut -c 1-7: Get the first 7 characters \\
sort: sort the sequences \\
uniq -c: Find the unique 7 characters are count them \\
sort -nr: sort the sequences and reverse the order \\
head -n10: Print the top 10 \\
\end{answer}
\end{questions}

\begin{steps}
Our barcodes are actually in a file called barcodes
\begin{lstlisting}
cd ~/rawData/Multiplexed/
gunzip Run1_R1.fastq.gz
gunzip Run1_R2.fastq.gz
sabre pe -m 1 -f Run1_R1.fastq -r Run1_R2.fastq -b barcodes_R1.txt \
-u unknown_barcode1.fastq -w unknown_barcode2.fastq
\end{lstlisting}
\end{steps}

\begin{questions}
How many read pairs were extracted in each sample? \\
\begin{answer}
Sample1: 1869572 reads (934786 pairs), Sample2: 2045180 reads (1022590 pairs)
\end{answer}
Run the command again without the one mismatch. How many read are now in each? \\
\begin{answer}
Sample1: 1831248 reads (915624 pairs), Sample2: 2027884 reads (1013942 pairs)
\end{answer}
\end{questions}

\section{Removal of Low Quality Reads and Adapters}

Adapter removal is an important step in many sequencing projects, mainly projects associated with small DNA/RNA inserts. For example, a common RNAseq experiment is to sequence small non-coding RNAs that are generated by an indvidual to regulate other coding sequences. These small RNAs (namely miRNAs, snoRNAs and/or siRNAs) are generally between 19-35bp, which is significantly smaller than the shortest, standard read length of most Illumina MiSeq/NextSeq/HiSeq machines (usually have read length settings such as 50, 75, 100, 125, 250 or 300bp). Therefore it is important to trim adapters accurately to ensure that the genome mapping and other downstream analyses are accurate.
In previous workshops we would run multiple steps to remove both low-quality reads, but today's trimming algorithms have become better at removing low-quality data and the same time as removing adapters. \\

The tool we'll use today is \texttt{cutadapt} \& it's one of the few bioinformatics tools to have a helpful webpage, so head to the site \url{http://cutadapt.readthedocs.org/}.


\subsection{Paired-End Data}
% Expand this seciton next time
\begin{advanced}
\begin{questions}
In the above process, we removed the adapters from each file separately.
What did the setting \texttt{-m 20} specify?\\
\begin{answer}
This told cutadapt to discard any reads shorter than 20nt after adapter removal.
\end{answer}
Could this cause any problems for paired-end reads, where each fastq file must contain exactly matching reads?\\
\begin{answer}
Yes. This would disrupt the matching structure of the files.
\end{answer}
\end{questions}

The more recent versions of \texttt{cutadapt} allow for trimming sets of paired reads.
Check the website for the correct code at \url{http://cutadapt.readthedocs.org/en/stable/guide.html#trimming-paired-end-reads}.
\end{advanced}

\begin{steps}
Now we can trim the data our demultiplexed data using the Illumina Nextera paired-end adapters. This are commonly used in Illumina projects. The index in the 3' adapter of the first read, which is used to demultiplex on the sequencing machine, we need block the sequence otherwise it can cause problems in the adapter matching. To do this, we use a sequence of "N" nucleotides.
\end{steps}

\begin{lstlisting}
cd ~/rawData/Multiplexed
mkdir -p ~/rawData/Multiplexed/trimmedData
cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCACNNNNNNATCTCGTATGCCGTCTTCTGCTTG \
    -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT \
    -o ~/rawData/Multiplexed/trimmedData/Sample1_r1_trim1.fastq \
    -p ~/rawData/Multiplexed/trimmedData/Sample1_r2_trim2.fastq \
    Sample1_r1.fastq Sample1_r2.fastq
\end{lstlisting}

Now lets do this with Sample2.

\begin{questions}
When running the adapter trimming we needed to add blocking "N" nucleotides to the adapter sequence. Why do we do this? \\
\begin{answer}
Because the indexes are different from project to project, we need to allow any nucleotide to match.
\end{answer}
How many final adapter trimmed reads were found in Sample1 and Sample2? \\
\begin{answer}
Sample1 produced 915,624 paired reads and Sample2 produced 1,013,942 paired reads.
\end{answer}
The cutadapt program produces a large amount of information about the trimming process, especially regarding the sequence length distribution of the output reads. To the nearest 10bp (i.e. 10-20 or 110-120), what would be the modal peak of each Sample (the 10bp containing the most number of sequences)?
\begin{answer}
Sample1: 80-90bp and Sample2: 60-70bp
\end{answer}
\end{questions}
